# -*- coding: utf-8 -*-
"""ProcessMining_app_v1.ipynb

Automatically generated by Colab.

"""

# Install necessary libraries
!pip install dash
!pip install plotly
!pip install pandas
!pip install flask
!pip install google-colab
!pip install pm4py

from datetime import datetime, timedelta  # Ensure timedelta is imported
import random

# Import necessary libraries
import pandas as pd
import random
import time
from datetime import datetime, timedelta
import plotly.express as px
from dash import Dash, dcc, html, Input, Output, dash_table
from flask import Flask
from google.colab.output import eval_js
import base64
import io

# =============== Generate a Simple Event Log for Testing =============== #
def generate_sample_event_log():
    cases = [f"Case_{i}" for i in range(1, 6)]  # 5 cases
    activities = ["Start", "Review", "Approve", "End"]

    data = []
    for case in cases:
        start_time = datetime.now() - timedelta(days=random.randint(1, 30))
        for activity in activities:
            data.append({
                "case_id": case,
                "activity": activity,
                "timestamp": start_time.strftime("%Y-%m-%d %H:%M:%S")
            })
            start_time += timedelta(minutes=random.randint(5, 60))  # Random delay

    return pd.DataFrame(data)

# =============== Data Validation Function =============== #
def validate_data(df):
    # Ensure timestamp is in datetime format
    try:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    except Exception as e:
        raise ValueError(f"Timestamp column conversion failed: {str(e)}")

    # Check for NaN or missing values in critical columns
    if df[['activity', 'case_id', 'timestamp']].isnull().any().any():
        raise ValueError("There are missing values in one of the required columns: 'activity', 'case_id', or 'timestamp'")

    return df

# =============== Decode File Function =============== #
def decode_file(contents):
    content_type, content_string = contents.split(',')
    decoded = base64.b64decode(content_string)
    return io.StringIO(decoded.decode('utf-8'))

# Generate sample event log
event_log = generate_sample_event_log()

# =============== Flask + Dash App Setup =============== #
server = Flask(__name__)  # Create Flask app
app = Dash(__name__, server=server)  # Dash app inside Flask

# =============== Dash Layout (User Interface) =============== #
app.layout = html.Div([
    html.H1("Process Mining Dashboard", style={'textAlign': 'center'}),

    # File Upload Section
    dcc.Upload(
        id='upload-data',
        children=html.Button('Upload CSV File'),
        multiple=False
    ),

    html.Hr(),

    # Data Overview Section
    html.Div(id='data-overview'),

    html.Hr(),

    # Process Analysis Tabs
    dcc.Tabs([
        dcc.Tab(label='Process Bottlenecks', children=[
            dcc.Graph(id='bottleneck-graph')
        ]),
        dcc.Tab(label='Process Loops', children=[
            html.Div(id='loop-summary')
        ]),
        dcc.Tab(label='Process Variants', children=[
            html.Div(id='variant-summary')
        ]),
        dcc.Tab(label='Compliance Analysis', children=[
            html.Div(id='compliance-summary')
        ])
    ])
])

# =============== Callbacks to Handle Data Upload and Analysis =============== #

# **1. Data Overview**
@app.callback(
    Output('data-overview', 'children'),
    Input('upload-data', 'contents')
)
def update_data_overview(contents):
    global event_log  # Use the generated sample log if no file is uploaded

    if contents is None:
        df = event_log
    else:
        # Decode and read the uploaded CSV
        file_io = decode_file(contents)
        df = pd.read_csv(file_io)

    try:
        df = validate_data(df)
    except ValueError as e:
        return html.Div([
            html.H3(f"Error: {str(e)}")
        ])

    return html.Div([
        html.H3("Data Overview"),
        html.P(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}"),
        html.P(f"Number of Cases: {df['case_id'].nunique()}"),
        html.P(f"Number of Activities: {df['activity'].nunique()}"),
        dash_table.DataTable(data=df.head(5).to_dict('records'), page_size=5)
    ])

# **2. Bottleneck Visualization**
@app.callback(
    Output('bottleneck-graph', 'figure'),
    Input('upload-data', 'contents')
)
def detect_bottlenecks(contents):
    df = event_log if contents is None else pd.read_csv(decode_file(contents))

    try:
        df = validate_data(df)
    except ValueError as e:
        return px.bar([], x=[], y=[], title=f"Error: {str(e)}")

    # Calculate average time spent per activity
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['time_diff'] = df.groupby('case_id')['timestamp'].diff().dt.total_seconds()

    # Convert time_diff to minutes and hours
    df['time_diff_minutes'] = df['time_diff'] / 60
    df['time_diff_hours'] = df['time_diff'] / 3600

    avg_times = df.groupby('activity')[['time_diff_minutes', 'time_diff_hours']].mean().reset_index()

    # Plotting the average time spent per activity (minutes and hours)
    fig = px.bar(avg_times, x='activity', y=['time_diff_minutes', 'time_diff_hours'],
                 title="Average Time Spent Per Activity",
                 labels={"activity": "Activity", "value": "Time", "variable": "Time Units"})

    return fig

# **3. Detect Process Loops**
@app.callback(
    Output('loop-summary', 'children'),
    Input('upload-data', 'contents')
)
def detect_loops(contents):
    df = event_log if contents is None else pd.read_csv(decode_file(contents))

    try:
        df = validate_data(df)
    except ValueError as e:
        return html.Div([
            html.H3(f"Error: {str(e)}")
        ])

    # Sort the dataframe by case_id and timestamp
    df = df.sort_values(by=['case_id', 'timestamp'])

    # Detect loops: activities repeated within a case
    loop_counts = df.groupby(['case_id', 'activity']).size().reset_index(name='count')
    loop_counts = loop_counts[loop_counts['count'] > 1]  # Only keep loops

    if loop_counts.empty:
        return html.Div([
            html.H3("No Loops Detected")
        ])

    # Group the loops by case_id and list involved activities
    loop_details = []
    for case_id, loop in loop_counts.groupby('case_id'):
        activities_involved = ", ".join(loop['activity'].unique())
        loop_details.append(html.Div([
            html.H4(f"Case: {case_id}"),
            html.P(f"Involved Activities: {activities_involved}")
        ]))

    return html.Div([
        html.H3("Process Loops"),
        *loop_details
    ])

# **4. Process Variants**
@app.callback(
    Output('variant-summary', 'children'),
    Input('upload-data', 'contents')
)
def analyze_variants(contents):
    df = event_log if contents is None else pd.read_csv(decode_file(contents))

    try:
        df = validate_data(df)
    except ValueError as e:
        return html.Div([
            html.H3(f"Error: {str(e)}")
        ])

    # Extract process variants
    variants = df.groupby('case_id')['activity'].apply(tuple).value_counts()

    if variants.empty:
        return html.Div([
            html.H3("No Process Variants Found")
        ])

    # Prepare data for DataTable
    variants_df = pd.DataFrame(variants).reset_index()
    variants_df.columns = ['Process Variant', 'Count']

    # Show top-10 variants
    top_10_variants = variants_df.head(10)

    variant_details = []
    for idx, row in top_10_variants.iterrows():
        variant_activities = " → ".join(row['Process Variant'])  # Get activities for each variant in the correct format
        variant_details.append(html.Div([
            html.H4(f"✅ Variant {idx + 1}: {variant_activities} (Count: {row['Count']})")
        ]))

    return html.Div([
        html.H3("Top 10 Process Variants"),
        *variant_details
    ])

# **5. Compliance Analysis**
@app.callback(
    Output('compliance-summary', 'children'),
    Input('upload-data', 'contents')
)
def compliance_analysis(contents):
    df = event_log if contents is None else pd.read_csv(decode_file(contents))

    try:
        df = validate_data(df)
    except ValueError as e:
        return html.Div([
            html.H3(f"Error: {str(e)}")
        ])

    # Define expected process sequence
    expected_sequence = ["Start", "Review", "Approve", "End"]

    # Compare each case sequence to expected
    df['process_sequence'] = df.groupby('case_id')['activity'].transform(lambda x: tuple(x))
    compliance_issues = df[~df['process_sequence'].isin([tuple(expected_sequence)])]

    return html.Div([
        html.H3("Compliance Issues"),
        html.P(f"Non-compliant cases: {compliance_issues['case_id'].nunique()}"),
        dash_table.DataTable(data=compliance_issues[['case_id', 'process_sequence']].drop_duplicates().to_dict('records'), page_size=5)
    ])

# =============== Run the App in Colab =============== #
if __name__ == '__main__':
    colab_url = eval_js("google.colab.kernel.proxyPort(8050)")
    print(f"Access the app here: {colab_url}")
    app.run_server(debug=True, port=8050, host='0.0.0.0')
